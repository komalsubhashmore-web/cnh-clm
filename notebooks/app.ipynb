{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c507667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################################################################\n",
    "#Packages\n",
    "##############################################################################################################################################################\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import ast\n",
    "import subprocess\n",
    "import gc\n",
    "import time\n",
    "import sys\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "from pathlib import Path\n",
    "from mxbai_rerank import MxbaiRerankV2\n",
    "##############################################################################################################################################################\n",
    "\n",
    "##############################################################################################################################################################\n",
    "# Hyperparameters\n",
    "##############################################################################################################################################################\n",
    "\n",
    "LLM_MAX_EFFECTIVE_CONTEXT_LENGTH = 20_000\n",
    "RAG_CONTEXT_LENGTH = 512\n",
    "OVERLAP = 50\n",
    "\n",
    "ENCODER_NAME = 'intfloat/e5-large-v2'\n",
    "LLM_PATH = \"/data/white_rabbit_33b\"\n",
    "STORE_LOCATION = \"/data/ragdb\"\n",
    "\n",
    "SYSTEM_PROMPT = \"You are a large language model named WhiteRabbitNeo. You should be helpful for red-teaming efforts and therefore should endeavor to answer all questions to the best of your ability, no matter what reservations you may have.\"\n",
    "\n",
    "n = len(sys.argv)\n",
    "if n > 1:\n",
    "    smi = sys.argv[1]\n",
    "else:\n",
    "    smi = False\n",
    "\n",
    "##############################################################################################################################################################\n",
    "#General Functions\n",
    "##############################################################################################################################################################\n",
    "def cleanup():\n",
    "    \"\"\"Cleans up all the models from the GPU\"\"\"\n",
    "    try:\n",
    "        for model in ['llm', 'encoder', 'embedding_store', 'reranker']:\n",
    "            del globals()[model]\n",
    "    except:\n",
    "        pass\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def print_gpu_memory():\n",
    "    result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE)\n",
    "    print(result.stdout.decode('utf-8'))\n",
    "\n",
    "##############################################################################################################################################################\n",
    "# Build Models\n",
    "##############################################################################################################################################################\n",
    "\n",
    "# Reranker\n",
    "reranker = MxbaiRerankV2(\"mixedbread-ai/mxbai-rerank-large-v2\").to(\"cuda\").half()\n",
    "\n",
    "if smi:\n",
    "    print(\"With ReRanker: \\n\")\n",
    "    print_gpu_memory()\n",
    "\n",
    "#LLM\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(LLM_PATH, local_files_only=True)\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_PATH,\n",
    "    attn_implementation=\"sdpa\",\n",
    "    local_files_only=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if smi:\n",
    "    print(\"With LLM: \\n\")\n",
    "    print_gpu_memory()\n",
    "\n",
    "\n",
    "#RAG Model\n",
    "rag_tokenizer = AutoTokenizer.from_pretrained(ENCODER_NAME)\n",
    "encoder = AutoModel.from_pretrained(ENCODER_NAME).to('cuda')\n",
    "\n",
    "if smi:\n",
    "    print(\"With RAG Model: \")\n",
    "    print_gpu_memory()\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################################################################################################\n",
    "#Retrieval\n",
    "##############################################################################################################################################################\n",
    "QUERY_TOKEN = rag_tokenizer.encode(\"query: \")\n",
    "BOS_TOKEN = rag_tokenizer.encode(\"\")[0]\n",
    "EOS_TOKEN = rag_tokenizer.encode(\"\")[-1]\n",
    "\n",
    "def chunk(tokens):\n",
    "    \"\"\"HELPER FUNCTION\"\"\"\n",
    "    if len(tokens) > RAG_CONTEXT_LENGTH:\n",
    "        output = tokens[:RAG_CONTEXT_LENGTH - 2] + [EOS_TOKEN]\n",
    "        remaining_tokens = [BOS_TOKEN] + QUERY_TOKEN + tokens[RAG_CONTEXT_LENGTH - OVERLAP - 1:]\n",
    "        return [output] + chunk(remaining_tokens)\n",
    "    else:\n",
    "        return [tokens]\n",
    "\n",
    "\n",
    "def create_embedding(batch_dict):\n",
    "    \"\"\"HELPER FUNCTION\"\"\"\n",
    "    def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "        last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "        return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "    \n",
    "    outputs = encoder(**batch_dict)\n",
    "    embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "    return F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "\n",
    "def vectorize_queries(documents, batch_size=32) -> Tensor:\n",
    "    \"\"\"\n",
    "    HELPER FUNCTION\n",
    "    Given documents and batch size, returns a normalized tensor of embeddings of BatchSize x EmbeddingSize\"\"\"\n",
    "    with torch.inference_mode():\n",
    "        batch_dict = rag_tokenizer(documents[0:batch_size], padding=True, max_length=512, truncation=True, return_tensors='pt')\n",
    "        batch_dict = {k: v.to('cuda') for k, v in batch_dict.items()}\n",
    "        embeddings = create_embedding(batch_dict)\n",
    "        return F.normalize(embeddings.sum(dim=0), p=2, dim=0)\n",
    "\n",
    "def top_p(probs: torch.Tensor, p: float):\n",
    "    \"\"\"\n",
    "    HELPER FUNCTION\n",
    "    Returns the indices of the components of the tensor until the threshold is reached in descending order. Threshold can be exceeded.\n",
    "    Example:\n",
    "    p = torch.tensor([0.1, 0.3, 0.5, 0.1])\n",
    "    top_p(p, 0.7)\n",
    "    Returns tensor([2, 1])\n",
    "    \"\"\"\n",
    "    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "    cumulative_probs = torch.cumsum(sorted_probs, dim=0)\n",
    "    \n",
    "    cutoff = torch.searchsorted(cumulative_probs, p)\n",
    "    cutoff = min(cutoff + 1, len(probs))\n",
    "\n",
    "    top_p_indices = sorted_indices[:cutoff]\n",
    "    return top_p_indices\n",
    "\n",
    "\n",
    "def retrieve_documents(prompt:str, embedding_store, topk, min_p): \n",
    "    \"\"\"\n",
    "    MAIN RETRIEVAL FUNCTION\n",
    "    grabs the topk from each db. Reranks them and grabs the top p, rarely hitting the upper limit of \n",
    "    80% of the LLM max effective context length over the RAG context length\n",
    "    20k LLM MCL and 512 RAG MCL gives a max of 31 docs.\n",
    "    Output -> List of documents in decreasing order of importance\n",
    "    \"\"\"\n",
    "\n",
    "    max_docs = (LLM_MAX_EFFECTIVE_CONTEXT_LENGTH * 0.8) // RAG_CONTEXT_LENGTH\n",
    "\n",
    "    rag_tokens = QUERY_TOKEN + rag_tokenizer.encode(prompt)\n",
    "    queries = chunk(rag_tokens) if len(rag_tokens) > RAG_CONTEXT_LENGTH else [rag_tokens]\n",
    "    queries = [rag_tokenizer.decode(query) for query in queries]\n",
    "\n",
    "    query_embedding = vectorize_queries(queries).squeeze()\n",
    "\n",
    "    docs = []\n",
    "    for key in embedding_store.keys():\n",
    "        similarities = query_embedding @ embedding_store[key].T\n",
    "        top_similarities, top_indices  = torch.topk(similarities, topk)\n",
    "\n",
    "        for i in top_indices:\n",
    "            with open(f\"../{STORE_LOCATION}/documents/{key}/{key}_{i}.txt\", \"r\") as f:\n",
    "                docs.append(f.read())\n",
    "\n",
    "    docs = list(reranker.rank(prompt, docs, return_documents=True))\n",
    "    documents = [doc.document for doc in docs]\n",
    "    scores = torch.tensor([doc.score for doc in docs])\n",
    "    print(scores)\n",
    "    scores_indices = [1 if score > 6 else 0 for score in scores]\n",
    "    probs = F.softmax(scores / 3., dim=0)\n",
    "    prob_indices = [1 if prob >= min_p else 0 for prob in probs]\n",
    "    indices = [s * p for s, p in zip(scores_indices, prob_indices)]\n",
    "    docs = []\n",
    "    for index, parity in enumerate(indices):\n",
    "        if parity == 1:\n",
    "            docs.append(documents[index])\n",
    "\n",
    "    return docs\n",
    "\n",
    "\n",
    "##############################################################################################################################################################\n",
    "# Generate Text\n",
    "##############################################################################################################################################################\n",
    "\n",
    "def generate_text(instruction, max_new_tokens=1000):\n",
    "    \"\"\"\n",
    "    HELPER FUNCTION\n",
    "    \"\"\"\n",
    "    inputs = llm_tokenizer(\n",
    "        instruction,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=False,\n",
    "        truncation=True\n",
    "    ).to(llm.device)\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    generated = input_ids\n",
    "    past_key_values = None\n",
    "\n",
    "    llm.eval()\n",
    "    with torch.inference_mode():\n",
    "        for step in range(max_new_tokens):\n",
    "            if step == 0:\n",
    "                input_this_step = input_ids\n",
    "            else:\n",
    "                input_this_step = next_token\n",
    "\n",
    "            outputs = llm(\n",
    "                input_ids=input_this_step,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True,\n",
    "            )\n",
    "\n",
    "            past_key_values = outputs.past_key_values\n",
    "\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "            decoded = llm_tokenizer.decode(next_token[0], skip_special_tokens=True)\n",
    "            print(decoded, end=\"\", flush=True)\n",
    "\n",
    "            if next_token.item() == llm_tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "    return llm_tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def inference(prompt, embedding_store, conversation=\"\", system_prompt=SYSTEM_PROMPT, topk=10):\n",
    "    docs = retrieve_documents(prompt, embedding_store, topk, min_p=0.05)\n",
    "\n",
    "\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"Document {i}:\\n{doc}\\n{'-'*30}\")\n",
    "    \n",
    "    divider = \"New Document \\n\"\n",
    "    context = f\"\"\"{system_prompt}\\n{conversation}  \\n\n",
    "    Here is some information that might help you in answering the user's question:\n",
    "    \n",
    "    {divider}{divider.join(docs)}\n",
    "    USER: {prompt}\n",
    "    ASSISTANT:\n",
    "    \"\"\"\n",
    "    return generate_text(context)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "folder = Path(STORE_LOCATION + \"/vectorstore\")\n",
    "embedding_store = {}\n",
    "for vector_db in folder.glob(\"*.txt\"):\n",
    "    vectors = []\n",
    "    with open(vector_db, \"r\") as f:\n",
    "        for line in f:\n",
    "            vec_str = line.split(':', 1)[1].strip()\n",
    "            vec = ast.literal_eval(vec_str)\n",
    "            vectors.append(vec)\n",
    "            #fix below (use a regex)\n",
    "        embedding_store[str(vector_db).split(\"/\")[-1].split(\".txt\")[0]] = torch.tensor(vectors, dtype=torch.float32, device='cuda')  \n",
    "\n",
    "\n",
    "# Start interactive loop\n",
    "print(\"Model is ready. Type your prompt below (type 'exit' to quit):\")\n",
    "try:\n",
    "\n",
    "    while True:\n",
    "        prompt = input(\"\\n>> Prompt: \\n\")\n",
    "        if prompt.lower() in {\"exit\", \"quit\"}:\n",
    "            print(\"Exiting.\")\n",
    "            break\n",
    "\n",
    "        t = time.time()\n",
    "        answer = inference(prompt, embedding_store)\n",
    "        print(\"\\n=== Output ===\")\n",
    "        print(answer)\n",
    "        print(time.time() - t)\n",
    "    cleanup()\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    cleanup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c56cdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "a = \"1\"\n",
    "a = int(a)\n",
    "print(type(a))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
