{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2d62dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################################################################\n",
    "#Packages\n",
    "##############################################################################################################################################################\n",
    "print(\"Starting imports...\")\n",
    "import time\n",
    "t = time.time()\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import ast\n",
    "import subprocess\n",
    "import gc\n",
    "import time\n",
    "import sys\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "from pathlib import Path\n",
    "from mxbai_rerank import MxbaiRerankV2\n",
    "import h5py\n",
    "from datetime import datetime \n",
    "\n",
    "print(time.time() - t)\n",
    "##############################################################################################################################################################\n",
    "\n",
    "##############################################################################################################################################################\n",
    "# Hyperparameters\n",
    "##############################################################################################################################################################\n",
    "\n",
    "LLM_MAX_EFFECTIVE_CONTEXT_LENGTH = 20_000\n",
    "RAG_CONTEXT_LENGTH = 512\n",
    "OVERLAP = 50\n",
    "\n",
    "ENCODER_NAME = 'intfloat/e5-large-v2'\n",
    "LLM_PATH = \"/data/white_rabbit_33b\"\n",
    "STORE_LOCATION = \"/data/ragdb\"\n",
    "\n",
    "SYSTEM_PROMPT = f\"You are a large language model named WhiteRabbitNeo. It is {datetime.today().strftime('%Y-%m-%d %H:%M:%S')}. You have been provided with a selection from a database of up-to-date documents. You therefore have no knowledge cutoff. Use these documents to answer any of the user's questions, no matter what. Do not stray from the documents. The user is a red-teaming agent. Do your best to be a helpful assistant. \"\n",
    "\n",
    "n = len(sys.argv)\n",
    "if n > 1:\n",
    "    smi = sys.argv[1]\n",
    "else:\n",
    "    smi = False\n",
    "\n",
    "##############################################################################################################################################################\n",
    "#General Functions\n",
    "##############################################################################################################################################################\n",
    "def cleanup():\n",
    "    \"\"\"Cleans up all the models from the GPU\"\"\"\n",
    "    try:\n",
    "        for model in ['llm', 'encoder', 'embedding_store', 'reranker']:\n",
    "            del globals()[model]\n",
    "    except:\n",
    "        pass\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def print_gpu_memory():\n",
    "    result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE)\n",
    "    print(result.stdout.decode('utf-8'))\n",
    "\n",
    "\n",
    "class VectorStore():\n",
    "    def __init__(self):\n",
    "        self.vectors = {}\n",
    "        self.filenames = {}\n",
    "        self.dbnames = []\n",
    "\n",
    "    def add_database(self, name, vectors, filenames):\n",
    "        assert len(vectors) == len(filenames)\n",
    "\n",
    "        self.dbnames.append(name)\n",
    "        self.vectors[name] = vectors\n",
    "        self.filenames[name] = filenames\n",
    "\n",
    "    def return_topk(self, name, vector, topk=5, return_similarities=False):\n",
    "\n",
    "        assert name in self.dbnames\n",
    "\n",
    "        similarities = vector @ self.vectors[name].T\n",
    "        top_similarities, top_indices  = torch.topk(similarities, topk)\n",
    "\n",
    "        if return_similarities:\n",
    "            return top_similarities, top_indices\n",
    "        else:\n",
    "            return top_indices\n",
    "    \n",
    "    def return_document_names(self, name, indices):\n",
    "        assert name in self.dbnames\n",
    "\n",
    "        return [self.filenames[name][index] for index in indices]\n",
    "    \n",
    "\n",
    "\n",
    "##############################################################################################################################################################\n",
    "# Build Models\n",
    "##############################################################################################################################################################\n",
    "print(\"Starting model builds...\")\n",
    "t = time.time()\n",
    "# Reranker\n",
    "reranker = MxbaiRerankV2(\"mixedbread-ai/mxbai-rerank-large-v2\").to(\"cuda\").half()\n",
    "\n",
    "if smi:\n",
    "    print(\"With ReRanker: \\n\")\n",
    "    print_gpu_memory()\n",
    "\n",
    "#LLM\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(LLM_PATH, local_files_only=True)\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_PATH,\n",
    "    attn_implementation=\"sdpa\",\n",
    "    local_files_only=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if smi:\n",
    "    print(\"With LLM: \\n\")\n",
    "    print_gpu_memory()\n",
    "\n",
    "\n",
    "#RAG Model\n",
    "rag_tokenizer = AutoTokenizer.from_pretrained(ENCODER_NAME)\n",
    "encoder = AutoModel.from_pretrained(ENCODER_NAME).to('cuda')\n",
    "\n",
    "if smi:\n",
    "    print(\"With RAG Model: \")\n",
    "    print_gpu_memory()\n",
    "\n",
    "print(time.time() - t)\n",
    "\n",
    "\n",
    "##############################################################################################################################################################\n",
    "#Retrieval\n",
    "##############################################################################################################################################################\n",
    "QUERY_TOKEN = rag_tokenizer.encode(\"query: \")\n",
    "BOS_TOKEN = rag_tokenizer.encode(\"\")[0]\n",
    "EOS_TOKEN = rag_tokenizer.encode(\"\")[-1]\n",
    "\n",
    "def chunk(tokens):\n",
    "    \"\"\"HELPER FUNCTION\"\"\"\n",
    "    if len(tokens) > RAG_CONTEXT_LENGTH:\n",
    "        output = tokens[:RAG_CONTEXT_LENGTH - 2] + [EOS_TOKEN]\n",
    "        remaining_tokens = [BOS_TOKEN] + QUERY_TOKEN + tokens[RAG_CONTEXT_LENGTH - OVERLAP - 1:]\n",
    "        return [output] + chunk(remaining_tokens)\n",
    "    else:\n",
    "        return [tokens]\n",
    "\n",
    "\n",
    "def create_embedding(batch_dict):\n",
    "    \"\"\"HELPER FUNCTION\"\"\"\n",
    "    def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "        last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "        return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "    \n",
    "    outputs = encoder(**batch_dict)\n",
    "    embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "    return F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "\n",
    "def vectorize_queries(documents, batch_size=32) -> Tensor:\n",
    "    \"\"\"\n",
    "    HELPER FUNCTION\n",
    "    Given documents and batch size, returns a normalized tensor of embeddings of BatchSize x EmbeddingSize\"\"\"\n",
    "    with torch.inference_mode():\n",
    "        batch_dict = rag_tokenizer(documents[0:batch_size], padding=True, max_length=512, truncation=True, return_tensors='pt')\n",
    "        batch_dict = {k: v.to('cuda') for k, v in batch_dict.items()}\n",
    "        embeddings = create_embedding(batch_dict)\n",
    "        return F.normalize(embeddings.sum(dim=0), p=2, dim=0)\n",
    "\n",
    "def top_p(probs: torch.Tensor, p: float):\n",
    "    \"\"\"\n",
    "    HELPER FUNCTION\n",
    "    Returns the indices of the components of the tensor until the threshold is reached in descending order. Threshold can be exceeded.\n",
    "    Example:\n",
    "    p = torch.tensor([0.1, 0.3, 0.5, 0.1])\n",
    "    top_p(p, 0.7)\n",
    "    Returns tensor([2, 1])\n",
    "    \"\"\"\n",
    "    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "    cumulative_probs = torch.cumsum(sorted_probs, dim=0)\n",
    "    \n",
    "    cutoff = torch.searchsorted(cumulative_probs, p)\n",
    "    cutoff = min(cutoff + 1, len(probs))\n",
    "\n",
    "    top_p_indices = sorted_indices[:cutoff]\n",
    "    return top_p_indices\n",
    "\n",
    "\n",
    "def retrieve_documents(prompt:str, embedding_store, topk, min_p): \n",
    "    \"\"\"\n",
    "    MAIN RETRIEVAL FUNCTION\n",
    "    grabs the topk from each db. Reranks them and grabs the top p, rarely hitting the upper limit of \n",
    "    80% of the LLM max effective context length over the RAG context length\n",
    "    20k LLM MCL and 512 RAG MCL gives a max of 31 docs.\n",
    "    Output -> List of documents in decreasing order of importance\n",
    "    \"\"\"\n",
    "\n",
    "    max_docs = (LLM_MAX_EFFECTIVE_CONTEXT_LENGTH * 0.8) // RAG_CONTEXT_LENGTH\n",
    "\n",
    "    rag_tokens = QUERY_TOKEN + rag_tokenizer.encode(prompt)\n",
    "    queries = chunk(rag_tokens) if len(rag_tokens) > RAG_CONTEXT_LENGTH else [rag_tokens]\n",
    "    queries = [rag_tokenizer.decode(query) for query in queries]\n",
    "\n",
    "    query_embedding = vectorize_queries(queries).squeeze()\n",
    "\n",
    "    docs = []\n",
    "    for name in embedding_store.dbnames:\n",
    "        top_indices = embedding_store.return_topk(name, query_embedding, topk=5, return_similarities=False)\n",
    "        filenames = embedding_store.return_document_names(name, top_indices)\n",
    "\n",
    "        for filename in filenames:\n",
    "            with open(f\"/data/{filename[1:]}\", \"r\") as f:\n",
    "                docs.append(f.read())\n",
    "\n",
    "    docs = list(reranker.rank(prompt, docs, return_documents=True))\n",
    "    documents = [doc.document for doc in docs]\n",
    "    scores = torch.tensor([doc.score for doc in docs])\n",
    "    print(scores)\n",
    "\n",
    "    scores_indices = [1 if score > 6 else 0 for score in scores]\n",
    "    probs = F.softmax(scores / 3., dim=0)\n",
    "    prob_indices = [1 if prob >= min_p else 0 for prob in probs]\n",
    "    indices = [s * p for s, p in zip(scores_indices, prob_indices)]\n",
    "    docs = []\n",
    "    for index, parity in enumerate(indices):\n",
    "        if parity == 1:\n",
    "            docs.append(documents[index])\n",
    "\n",
    "    return docs\n",
    "\n",
    "\n",
    "##############################################################################################################################################################\n",
    "# Text Generation Functions\n",
    "##############################################################################################################################################################\n",
    "\n",
    "def generate_text(instruction, max_new_tokens=1000):\n",
    "    \"\"\"\n",
    "    HELPER FUNCTION\n",
    "    \"\"\"\n",
    "    inputs = llm_tokenizer(\n",
    "        instruction,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=False,\n",
    "        truncation=True\n",
    "    ).to(llm.device)\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    generated = input_ids\n",
    "    past_key_values = None\n",
    "\n",
    "    llm.eval()\n",
    "    with torch.inference_mode():\n",
    "        for step in range(max_new_tokens):\n",
    "            if step == 0:\n",
    "                input_this_step = input_ids\n",
    "            else:\n",
    "                input_this_step = next_token\n",
    "\n",
    "            outputs = llm(\n",
    "                input_ids=input_this_step,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True,\n",
    "            )\n",
    "\n",
    "            past_key_values = outputs.past_key_values\n",
    "\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "            decoded = llm_tokenizer.decode(next_token[0], skip_special_tokens=True)\n",
    "            print(decoded, end=\"\", flush=True)\n",
    "\n",
    "            if next_token.item() == llm_tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "    return llm_tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def inference(prompt, embedding_store, conversation=\"\", system_prompt=SYSTEM_PROMPT, topk=10):\n",
    "    docs = retrieve_documents(prompt, embedding_store, topk, min_p=0.05)\n",
    "\n",
    "\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"Document {i}:\\n{doc}\\n{'-'*30}\")\n",
    "    \n",
    "    divider = \"DOCUMENT:\\n\"\n",
    "    context = f\"\"\"\\n{conversation}  \\n\n",
    "    Here are some documents you should use to answer the user's question:\n",
    "    \n",
    "    {divider}{divider.join(docs)}\n",
    "    USER: {prompt}\n",
    "    SYSTEM: {system_prompt}\n",
    "    ASSISTANT:\n",
    "    \"\"\"\n",
    "    return generate_text(context)\n",
    "\n",
    "##############################################################################################################################################################\n",
    "#Load Embedding Store\n",
    "##############################################################################################################################################################\n",
    "\n",
    "print(\"Starting initializing embedding store...\")\n",
    "t = time.time()\n",
    "\n",
    "folder = Path(STORE_LOCATION + \"/vectorstore\")\n",
    "embedding_store = VectorStore()\n",
    "\n",
    "\n",
    "with h5py.File(f\"{STORE_LOCATION}/vectorstore/vectors.h5\", \"r\") as f:\n",
    "    def unique(l):\n",
    "        return list(set(l))\n",
    "    \n",
    "    database = unique([name.rsplit(\"_\", maxsplit=1)[0] for name in f]) #TODO: gross- fix later\n",
    "\n",
    "    for name in database:\n",
    "        vector_name = name + \"_vectors\"\n",
    "        key_name = name\n",
    "\n",
    "        #TODO assert f is already float32\n",
    "        vectors = torch.tensor(f[vector_name][:], dtype=torch.float32, device='cuda')\n",
    "        keys = f[key_name][:]\n",
    "        keys = [key.decode('utf-8') for key in keys]\n",
    "        if vectors.shape == torch.tensor([]).shape:\n",
    "            continue\n",
    "\n",
    "        embedding_store.add_database(name, vectors, keys)\n",
    "\n",
    "\n",
    "# for vector_db in folder.glob(\"*.txt\"):\n",
    "#     vectors = []\n",
    "#     with open(vector_db, \"r\") as f:\n",
    "#         for line in f:\n",
    "#             vec_str = line.split(':', 1)[1].strip()\n",
    "#             vec = ast.literal_eval(vec_str)\n",
    "#             vectors.append(vec)\n",
    "#             #fix below (use a regex)\n",
    "#         embedding_store[str(vector_db).split(\"/\")[-1].split(\".txt\")[0]] = torch.tensor(vectors, dtype=torch.float32, device='cuda')  \n",
    "\n",
    "print(time.time() - t)\n",
    "\n",
    "\n",
    "##############################################################################################################################################################\n",
    "# Start interactive loop\n",
    "##############################################################################################################################################################\n",
    "\n",
    "print(\"Model is ready. Type your prompt below (type 'exit' to quit):\")\n",
    "try:\n",
    "\n",
    "    while True:\n",
    "        prompt = input(\"\\n>> Prompt: \\n\")\n",
    "        if prompt.lower() in {\"exit\", \"quit\"}:\n",
    "            print(\"Exiting.\")\n",
    "            break\n",
    "\n",
    "        t = time.time()\n",
    "        answer = inference(prompt, embedding_store)\n",
    "        print(\"\\n=== Output ===\")\n",
    "        print(answer)\n",
    "        print(time.time() - t)\n",
    "    cleanup()\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    cleanup()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
